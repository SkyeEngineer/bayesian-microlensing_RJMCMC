<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>bayelens.sampling API documentation</title>
<meta name="description" content="Adaptive Reversible-Jump Metropolis Hastings for microlensing â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>bayelens.sampling</code></h1>
</header>
<section id="section-intro">
<p>Adaptive Reversible-Jump Metropolis Hastings for microlensing.</p>
<p>Implements algorithms for bayesian sampling. Uses the main
classes: State, Chain, and model.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Adaptive Reversible-Jump Metropolis Hastings for microlensing.

Implements algorithms for bayesian sampling. Uses the main 
classes: State, Chain, and model.
&#34;&#34;&#34;

import math
import random
import numpy as np
from scipy.stats import multivariate_normal
from copy import deepcopy
from types import MethodType


class State(object):
    &#34;&#34;&#34;State sampled from a model&#39;s probability distribution.

    Describes a point in both scaled and unscaled space. The scaling is 
    hardcoded but can be extended per application. Currently log10 scaling 
    the fourth parameter. In microlensing applications, this is q,
    the mass ratio.

    Attributes:
        truth: [list] Parameter values for the state, in true space.
        scaled: [list] Parameter values for the state, in scaled space.
        D: [int] The dimensionality of the state.
    &#34;&#34;&#34;

    def __init__(self, truth = None, scaled = None):
        &#34;&#34;&#34;Initialises state with truth, scaled, and D values.
        
        Only one of truth or scaled is needed.
        &#34;&#34;&#34;        
        if truth is not None:
            self.truth = truth
            self.D = len(truth)

            self.scaled = deepcopy(self.truth)
            for p in range(self.D):
                if p == 3:
                    self.scaled[p] = np.log10(self.truth[p])
        
        elif scaled is not None:
            self.scaled = scaled
            self.D = len(scaled)

            self.truth = deepcopy(self.scaled)
            for p in range(self.D):
                if p == 3:
                    self.truth[p] = 10**(self.scaled[p])

        else:   raise ValueError(&#34;Assigned null state&#34;)


class Chain(object):
    &#34;&#34;&#34;Collection of states.

    Describes a markov chain, perhaps from a joint model space.

    Attributes:
        states: [list] State objects in the chain.
        model_indices: [list] models the states are from. 
        n: [int] The number of states in the chain.
    &#34;&#34;&#34;

    def __init__(self, m, state):
        &#34;&#34;&#34;Initialises the chain with one state from one model.

        Args:
            state: [state] The state object.
            m: [int] The index of the model the state is from.
        &#34;&#34;&#34;
        self.states = [state]
        self.model_indices = [m]
        self.n = 1

    def add_general_state(self, m, state):
        &#34;&#34;&#34;Adds a state in a model to the chain.

        Args:
            state: [state] The state object.
            m: [int] The index of the model the state is from.
        &#34;&#34;&#34;
        self.states.append(state)
        self.model_indices.append(m)
        self.n += 1
        return

    def states_array(self, scaled = True):
        &#34;&#34;&#34;Creates a numpy array of all states in the chain.

        Args:
            scale: [optional, bool] Whether the array should be in scaled or 
                                    true space.

        Returns:
            chain_array: [np.array] The numpy array of all state parameters. 
                                    Columns are states, rows are parameters 
                                    for all states.
        &#34;&#34;&#34;
        n_states = len(self.states)
        D_state = len(self.states[-1].scaled)
        
        chain_array = np.zeros((D_state, n_states))

        if scaled:
            for i in range(n_states):
                chain_array[:, i] = self.states[i].scaled

        else:
            for i in range(n_states):
                chain_array[:, i] = self.states[i].truth

        return chain_array


class Model(object):
    &#34;&#34;&#34;A model to describe a probability distribution.

    Contains a chain of states from this model, as well as information
    from this. Adapts a covariance matrix iteratively with each new state,
    and stores a guess at a maximum posterior density estimate.

    Attributes:
        m: [int] Model index.
        D: [int] Dimensionality of a state in the model.
        priors: [list] Prior distribution objects for state parameter values.
        sampled: [chain] States sampled from the model&#39;s distribution.
        scaled_average_state: [list] The scaled average parameter values of the chain.
        centre: [state] Best guess at maximum posterior density.
        covariance: [array] Current covariance matrix, based on all states.
        covariances: [list] All previous covariance matrices.
        acc: [list] Binary values, 1 if the state proposed was accepted,
                    0 if it was rejected.
        data: [mulensdata] Object for photometry readings from the 
            microlensing event.
        log_likelihood: [function] Method to calculate the log likelihood a state is
                                    from this model.
        I: [np.array] Identity matrix the size of D.
        s: [float] Mixing parameter (see Haario et al 2001).
    &#34;&#34;&#34;

    def __init__(self, m, D, centre, priors, covariance, data, log_likelihood_fnc):
        &#34;&#34;&#34;Initialises the model.&#34;&#34;&#34;
        self.m = m
        self.D = D
        self.priors = priors
        self.centre = centre
        self.sampled = Chain(m, centre)
        self.scaled_avg_state = centre.scaled
        self.acc = [1] # First state always accepted.
        self.covariance = covariance
        self.covariances = [covariance]

        self.data = data
        
        # Model&#39;s custom likelihood function.
        self.log_likelihood = MethodType(log_likelihood_fnc, self)

        self.I = np.identity(D)
        self.s = 2.4**2 / D # Arbitrary(ish), good value from Haario et al 2001.
    
    def add_state(self, theta, adapt = True):
        &#34;&#34;&#34;Adds a sampled state to the model.

        Args:
            theta: [state] Parameters to add.
            adapt: [optional, bool] Whether or not to adjust the covariance 
                                    matrix based on the new state.
        &#34;&#34;&#34;
        self.sampled.n += 1
        self.sampled.states.append(theta)

        if adapt:
            self.covariance = iterative_covariance(self.covariance, theta.scaled, self.scaled_avg_state, self.sampled.n, self.s, self.I)

        self.covariances.append(self.covariance)
        self.scaled_avg_state = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n)
        self.centre = State(scaled = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n))

        return

    def log_prior_density(self, theta, v = None, v_D = None):
        &#34;&#34;&#34;Calculates the log prior density of a state in the model.

        Optionally adjusts this log density when using auxilliary vriables.

        Args:
            theta: [state] Parameters to calculate the log prior density for.
            v: [optional, state] The values of all auxiliary variables.
            v_D: [optional, int] The dimensionality to use with auxilliary variables.

        Returns:
            log_prior_product: [float] The log prior probability density.
        &#34;&#34;&#34;    
        log_prior_product = 0.

        # cycle through parameters
        for p in range(self.D):

            # product using log rules
            log_prior_product += (self.priors[p].log_pdf(theta.truth[p]))

        # cycle through auxiliary parameters if v and v_D passed
        if v is not None or v_D is not None:
            if v is not None and v_D is not None:
                for p in range(self.D, v_D):
                    
                    # product using log rules
                    log_prior_product += (self.priors[p].log_pdf(v.truth[p]))

            else: raise ValueError(&#34;Only one of v or v_D passed.&#34;)

        return log_prior_product


def iterative_mean(x_mu, x, n):
    return (x_mu * n + x)/(n + 1)

def iterative_covariance(cov, x, x_mu, n, s, I, eps = 1e-12):
    return (n-1)/n * cov + s/(n+1) * np.outer(x - x_mu, x - x_mu) + s*eps*I/n

def check_symmetric(A, tol = 1e-16):
    return np.all(np.abs(A-A.T) &lt; tol)



def gaussian_proposal(theta, covariance):
    &#34;&#34;&#34;Samples a gaussian move.&#34;&#34;&#34;
    return multivariate_normal.rvs(mean = theta, cov = covariance)

def AMH(model, adaptive_iterations, fixed_iterations = 25, user_feedback = False):
    &#34;&#34;&#34;Performs Adaptive Metropolis Hastings.
    
    Produces a posterior distribution by adapting the proposal process within 
    one model, as described in Haario et al (2001).

    Args:
        model: [model] Model object to sample the distribution from.
        adaptive_iterations: [int] Number of steps with adaption.
        fixed_iterations: [optional, int] Number of steps without adaption.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        best_theta: [state] State producing the best posterior density visited.
        log_best_posterior: [float] Best log posterior density visited. 
    &#34;&#34;&#34;

    if fixed_iterations &lt; 5:
        raise ValueError(&#34;Not enough iterations to safely establish an empirical covariance matrix.&#34;)
    

    theta = model.centre
    best_theta = deepcopy(theta)

    # Initial propbability values.
    log_likelihood = model.log_likelihood(theta)
    log_prior = model.log_prior_density(theta)
    log_best_posterior = log_likelihood + log_prior

    # Warm up walk to establish an empirical covariance.
    for i in range(1, fixed_iterations):

        # Propose a new state and calculate the resulting density.
        proposed = State(scaled = gaussian_proposal(theta.scaled, model.covariance))
        log_likelihood_proposed = model.log_likelihood(proposed)
        log_prior_proposed = model.log_prior_density(proposed)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            theta = deepcopy(proposed)
            log_likelihood = log_likelihood_proposed
            model.acc.append(1)

            # Store best state.
            log_posterior = log_likelihood_proposed + log_prior_proposed
            if log_best_posterior &lt; log_posterior:
                log_best_posterior = log_posterior
                best_theta = deepcopy(theta)

        else: model.acc.append(0) # Reject proposal.
        
        # Update storage.
        model.add_state(theta, adapt = False)

    # Calculate intial empirical covariance matrix.
    model.covariance = np.cov(model.sampled.states_array(scaled = True))
    model.covariances.pop()
    model.covariances.append(model.covariance)

    # Perform adaptive walk.
    for i in range(fixed_iterations, adaptive_iterations + fixed_iterations):

        if user_feedback:
            cf = i / (adaptive_iterations + fixed_iterations - 1)
            print(f&#39;log score: {log_best_posterior:.4f}, progress: [{&#34;#&#34;*round(50*cf)+&#34;-&#34;*round(50*(1-cf))}] {100.*cf:.2f}%\r&#39;, end=&#34;&#34;)

        # Propose a new state and calculate the resulting density.
        proposed = State(scaled = gaussian_proposal(theta.scaled, model.covariance))
        log_likelihood_proposed = model.log_likelihood(proposed)
        log_prior_proposed = model.log_prior_density(proposed)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            theta = deepcopy(proposed)
            log_likelihood = log_likelihood_proposed
            model.acc.append(1)

            # Store the best state.
            log_posterior = log_likelihood_proposed + log_prior_proposed
            if log_best_posterior &lt; log_posterior:
                log_best_posterior = log_posterior
                best_theta = deepcopy(theta)

        else: model.acc.append(0) # Reject proposal.
        
        # Update model chain.
        model.add_state(theta, adapt = True)

    if user_feedback:
        print(f&#34;\n model: {model.m}, average acc: {(np.sum(model.acc) / (adaptive_iterations + fixed_iterations)):4f}, best score: {log_best_posterior:.4f}&#34;)

    return best_theta, log_best_posterior


def ARJMH_proposal(model, proposed_model, theta, lv):
    &#34;&#34;&#34;Performs an Adaptive Reversible-Jump Metropolis Hastings proposal.
    
    Args:
        model: [model] Model to jump from.
        proposed_model: [model] Model to jump to.
        theta: [state] State to jump from.
        lv: [list] Current auxilliary variables centre divergence.

    Returns:
        proposed_theta: [state] State proposed to jump to.
    &#34;&#34;&#34;
    l = theta.scaled - model.centre.scaled # Offset from initial model&#39;s centre.

    if model is proposed_model: # Intra-model move.

        # Use the covariance at the proposed model&#39;s centre for local shape.
        u = gaussian_proposal(np.zeros((proposed_model.D)), proposed_model.covariance)
        proposed_theta = u + l + proposed_model.centre.scaled
        
        return proposed_theta

    else: # Inter-model move.
        
        s = abs(model.D - proposed_model.D) # Subset size.

        # Use superset model covariance
        if proposed_model.D &gt; model.D: # proposed is superset
            cov = proposed_model.covariance
        else: # proposed is subset
            cov = model.covariance

        conditioned_cov = schur_complement(cov, s)

        # Jump to smaller model. Fix non-shared parameters.
        if proposed_model.D &lt; model.D:

            u = gaussian_proposal(np.zeros((s)), conditioned_cov)
            proposed_theta = u + l[:s] + proposed_model.centre.scaled

            return proposed_theta

        if proposed_model.D &gt; model.D: # Jump to larger model. Append v.

            u = gaussian_proposal(np.zeros((s)), conditioned_cov)
            shared_map = u + l[:s] + proposed_model.centre.scaled[:s]
            non_shared_map = lv[s:] + proposed_model.centre.scaled[s:]
            map = np.concatenate((shared_map, non_shared_map))
            proposed_theta = map

            return proposed_theta

def schur_complement(cov, s):
    c_11 = cov[:s, :s] # Covariance matrix of shared parameters.
    c_12 = cov[:s, s:] # Covariances, not variances.
    c_21 = cov[s:, :s] # Same as above.
    c_22 = cov[s:, s:] # Covariance matrix of non-shared.
    c_22_inv = np.linalg.inv(c_22)

    return c_11 - c_12.dot(c_22_inv).dot(c_21)

def warm_up_model(empty_model, adaptive_iterations, fixed_iterations = 25, repetitions = 1, user_feedback = False):
    &#34;&#34;&#34;Prepares a model for the ARJMH algorithm.
    
    Repeats the adaptive MH warmup process for a model, storing the best run.

    Args:
        empty_model: [model] Initial model object.
        adaptive_iterations: [int] Number of adaptive steps.
        fixed_iterations: [optional, int] Number of non-adaptive steps.
        repetitions: [optional, int] Number of times to try for a better run.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        incumbent_model: [model] Model with the states from the best run.
    &#34;&#34;&#34;

    inc_log_best_posterior = -math.inf # Initialise incumbent posterior to always lose.

    for i in range(repetitions):
        
        if user_feedback:
            print(&#34;Running the &#34;+str(i+1)+&#34;/&#34;+str(repetitions)+&#34;th initialisation per model\n&#34;)

        model = deepcopy(empty_model) # Fresh model.

        # Run adaptive MH.
        best_theta, log_best_posterior = AMH(model, adaptive_iterations, fixed_iterations = fixed_iterations, user_feedback = user_feedback)

        # Keep the best posterior density run.
        if inc_log_best_posterior &lt; log_best_posterior:
            incumbent_model = deepcopy(model)
            incumbent_model.centre = deepcopy(best_theta)

    return incumbent_model


def ARJMH(models, iterations,  adaptive_warm_up_iterations, fixed_warm_up_iterations = 25, warm_up_repititions = 1, user_feedback = False):
    &#34;&#34;&#34;Samples from a joint distribution of models.
    
    Initialises each model with multiple adaptive MH runs. Then uses the resulting
    covariances to run adaptive RJMH on all models.

    Args:
        models: [list] Model objects to sample from. 
                    Should be sorted by increasing dimensionality.
        iterations: [int] Number of adaptive RJMH steps.
        adaptive_warm_up_iterations: [int] Number of adaptive steps to initialise with.
        fixed_warm_up_iterations: [int] Number of fixed steps to initilaise with.
        warm_up_repititions: [int] Number of times to try for a better initial run.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        joint_model_chain: [chain] Generalised chain with states from any model.
        total_acc: [list] Binary values, 1 if the state proposed was accepted,
                          0 if it was rejected, associated with the joint model.
        inter_model_history: [model] Stored inter-model covariances and acc.
    &#34;&#34;&#34;

    if len(models) == 2:
        inter_model_history = deepcopy(models[1])
        inter_model_history.covariances = [schur_complement(models[1].covariance, models[0].D)]
    else: inter_model_history = None

    # Initialise model chains.
    for m_i in range(len(models)):
        models[m_i] = warm_up_model(models[m_i], adaptive_warm_up_iterations, fixed_warm_up_iterations, warm_up_repititions, user_feedback)

    random.seed(42)

    # Choose a random model to start in.
    model = random.choice(models)
    theta = deepcopy(model.sampled.states[-1]) # Final state in model&#39;s warmup chain.

    v = deepcopy(models[-1].sampled.states[-1]) # Auxiliary variables final state in super set model.
    lv = models[-1].sampled.states[-1].scaled - models[-1].centre.scaled # Auxiliary variables offset from centre.

    # Create joint model as initial theta appended to auxiliary variables.
    initial_superset = models[-1].D - model.D
    if initial_superset &gt; 0: # If random choice was a subset model
        theta_v = np.concatenate((theta.scaled, models[-1].sampled.states[-1].scaled[model.D:]))
        joint_model_chain = Chain(model.m, State(scaled = theta_v))
    else:
        joint_model_chain = Chain(model.m, theta)

    total_acc = np.zeros(iterations)
    total_acc[0] = 1

    v_D = models[-1].D # Dimension of largest model is auxilliary variable size.

    # Initial probability values.
    log_likelihood = model.log_likelihood(theta)
    log_prior = model.log_prior_density(theta, v = v, v_D = v_D)


    if user_feedback: print(&#34;Running ARJMH.&#34;)
    for i in range(1, iterations): # ARJMH algorithm.
        
        if user_feedback:
            cf = i / (iterations - 1)
            print(f&#39;model: {model.m} progress: [{&#34;#&#34;*round(50*cf)+&#34;-&#34;*round(50*(1-cf))}] {100.*cf:.2f}%\r&#39;, end=&#34;&#34;)

        # Propose a new model and state and calculate the resulting density.
        proposed_model = random.choice(models)
        proposed = State(scaled = ARJMH_proposal(model, proposed_model, theta, lv))
        log_likelihood_proposed = proposed_model.log_likelihood(proposed)
        log_prior_proposed = proposed_model.log_prior_density(proposed, v = v, v_D = v_D)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            total_acc[i] = 1

            if model is proposed_model: # Intra model move.
                model.acc.append(1)
            elif inter_model_history is not None: # Inter model move.
                inter_model_history.acc.append(1)
                inter_model_history.covariances.append(schur_complement(models[1].covariance, models[0].D))
                inter_model_history.sampled.n += 1

            model = proposed_model
            theta = deepcopy(proposed)

            log_likelihood = log_likelihood_proposed
            log_prior = log_prior_proposed
            
        else: # Reject proposal.
            total_acc[i] = 0
            
            if model is proposed_model: # Intra model move.
                model.acc.append(0)
            elif inter_model_history is not None: # Inter model move.
                inter_model_history.acc.append(0)
                inter_model_history.covariances.append(schur_complement(models[1].covariance, models[0].D))
                inter_model_history.sampled.n += 1
        
        # Update model chain.
        model.add_state(theta, adapt = True)
        v = State(scaled = np.concatenate((theta.scaled, v.scaled[model.D:])))
        joint_model_chain.add_general_state(model.m, v)

        # Update auxilliary centre divergence for new states.
        lv[:model.D] = theta.scaled - model.centre.scaled

    if user_feedback:
        print(f&#34;\n average acc: {np.average(total_acc):4f}&#34;)
        print(&#34;P(m1|y): &#34; + str(1 - np.sum(joint_model_chain.model_indices) / iterations))
        print(&#34;P(m2|y): &#34; + str(np.sum(joint_model_chain.model_indices) / iterations))

    return joint_model_chain, total_acc, inter_model_history



def output_file(models, warm_up_iterations, joint_model_chain, total_acc, n_epochs, sn, letters, name = &#34;&#34;, event_params = None):
    
    # Output File.
    with open(&#34;results/&#34;+name+&#34;-run.txt&#34;, &#34;w&#34;) as file:
        file.write(&#34;Run &#34;+name+&#34;\n&#34;)
        
        # Inputs.
        file.write(&#34;Inputs:\n&#34;)
        if event_params is not None:
            file.write(&#34;Parameters: &#34;+str(event_params.truth)+&#34;\n&#34;)
        file.write(&#34;Number of observations: &#34;+str(n_epochs)+&#34;, Signal to noise baseline: &#34;+str(sn)+&#34;\n&#34;)
        file.write(&#34;\n&#34;)
        file.write(&#34;Run information:\n&#34;)
        file.write(&#34;Iterations: &#34;+str(joint_model_chain.n)+&#34;\n&#34;)
        file.write(&#34;Average acc; Total: &#34;+str(np.average(total_acc)))

        # Results.
        file.write(&#34;\n\nResults:\n&#34;)
        for model in models:
            
            # Models.
            P_model = (model.sampled.n-warm_up_iterations)/joint_model_chain.n
            sde_model = np.std(np.array(joint_model_chain.model_indices))/(joint_model_chain.n**0.5)
            file.write(&#34;\n&#34;+str(model.m)+&#34;\nP(m|y): &#34;+str(P_model)+r&#34;\pm&#34;+str(sde_model)+&#34;\n&#34;)

            # Parameters.
            model_states = model.sampled.states_array(scaled = True)
            for i in range(len(model.sampled.states[-1].scaled)):
                mu = np.average(model_states[i, :])
                sd = np.std(model_states[i, :], ddof = 1)
                file.write(letters[i]+&#34;: mean: &#34;+str(mu)+&#34;, sd: &#34;+str(sd)+&#34; \n&#34;)
                
                # Hardcoded unscale q.
                if i == 3:
                    exp10 = np.power(10, model_states[i, :])
                    mu = np.average(exp10)
                    sd = np.std(exp10, ddof = 1)
                    file.write(&#34;q: mean: &#34;+str(mu)+&#34;, sd: &#34;+str(sd)+&#34; \n&#34;)

    return</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="bayelens.sampling.AMH"><code class="name flex">
<span>def <span class="ident">AMH</span></span>(<span>model, adaptive_iterations, fixed_iterations=25, user_feedback=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs Adaptive Metropolis Hastings.</p>
<p>Produces a posterior distribution by adapting the proposal process within
one model, as described in Haario et al (2001).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>[model] Model object to sample the distribution from.</dd>
<dt><strong><code>adaptive_iterations</code></strong></dt>
<dd>[int] Number of steps with adaption.</dd>
<dt><strong><code>fixed_iterations</code></strong></dt>
<dd>[optional, int] Number of steps without adaption.</dd>
<dt><strong><code>user_feedback</code></strong></dt>
<dd>[optional, bool] Whether or not to print progress.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>best_theta</code></dt>
<dd>[state] State producing the best posterior density visited.</dd>
<dt><code>log_best_posterior</code></dt>
<dd>[float] Best log posterior density visited.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def AMH(model, adaptive_iterations, fixed_iterations = 25, user_feedback = False):
    &#34;&#34;&#34;Performs Adaptive Metropolis Hastings.
    
    Produces a posterior distribution by adapting the proposal process within 
    one model, as described in Haario et al (2001).

    Args:
        model: [model] Model object to sample the distribution from.
        adaptive_iterations: [int] Number of steps with adaption.
        fixed_iterations: [optional, int] Number of steps without adaption.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        best_theta: [state] State producing the best posterior density visited.
        log_best_posterior: [float] Best log posterior density visited. 
    &#34;&#34;&#34;

    if fixed_iterations &lt; 5:
        raise ValueError(&#34;Not enough iterations to safely establish an empirical covariance matrix.&#34;)
    

    theta = model.centre
    best_theta = deepcopy(theta)

    # Initial propbability values.
    log_likelihood = model.log_likelihood(theta)
    log_prior = model.log_prior_density(theta)
    log_best_posterior = log_likelihood + log_prior

    # Warm up walk to establish an empirical covariance.
    for i in range(1, fixed_iterations):

        # Propose a new state and calculate the resulting density.
        proposed = State(scaled = gaussian_proposal(theta.scaled, model.covariance))
        log_likelihood_proposed = model.log_likelihood(proposed)
        log_prior_proposed = model.log_prior_density(proposed)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            theta = deepcopy(proposed)
            log_likelihood = log_likelihood_proposed
            model.acc.append(1)

            # Store best state.
            log_posterior = log_likelihood_proposed + log_prior_proposed
            if log_best_posterior &lt; log_posterior:
                log_best_posterior = log_posterior
                best_theta = deepcopy(theta)

        else: model.acc.append(0) # Reject proposal.
        
        # Update storage.
        model.add_state(theta, adapt = False)

    # Calculate intial empirical covariance matrix.
    model.covariance = np.cov(model.sampled.states_array(scaled = True))
    model.covariances.pop()
    model.covariances.append(model.covariance)

    # Perform adaptive walk.
    for i in range(fixed_iterations, adaptive_iterations + fixed_iterations):

        if user_feedback:
            cf = i / (adaptive_iterations + fixed_iterations - 1)
            print(f&#39;log score: {log_best_posterior:.4f}, progress: [{&#34;#&#34;*round(50*cf)+&#34;-&#34;*round(50*(1-cf))}] {100.*cf:.2f}%\r&#39;, end=&#34;&#34;)

        # Propose a new state and calculate the resulting density.
        proposed = State(scaled = gaussian_proposal(theta.scaled, model.covariance))
        log_likelihood_proposed = model.log_likelihood(proposed)
        log_prior_proposed = model.log_prior_density(proposed)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            theta = deepcopy(proposed)
            log_likelihood = log_likelihood_proposed
            model.acc.append(1)

            # Store the best state.
            log_posterior = log_likelihood_proposed + log_prior_proposed
            if log_best_posterior &lt; log_posterior:
                log_best_posterior = log_posterior
                best_theta = deepcopy(theta)

        else: model.acc.append(0) # Reject proposal.
        
        # Update model chain.
        model.add_state(theta, adapt = True)

    if user_feedback:
        print(f&#34;\n model: {model.m}, average acc: {(np.sum(model.acc) / (adaptive_iterations + fixed_iterations)):4f}, best score: {log_best_posterior:.4f}&#34;)

    return best_theta, log_best_posterior</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.ARJMH"><code class="name flex">
<span>def <span class="ident">ARJMH</span></span>(<span>models, iterations, adaptive_warm_up_iterations, fixed_warm_up_iterations=25, warm_up_repititions=1, user_feedback=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Samples from a joint distribution of models.</p>
<p>Initialises each model with multiple adaptive MH runs. Then uses the resulting
covariances to run adaptive RJMH on all models.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>models</code></strong></dt>
<dd>[list] Model objects to sample from.
Should be sorted by increasing dimensionality.</dd>
<dt><strong><code>iterations</code></strong></dt>
<dd>[int] Number of adaptive RJMH steps.</dd>
<dt><strong><code>adaptive_warm_up_iterations</code></strong></dt>
<dd>[int] Number of adaptive steps to initialise with.</dd>
<dt><strong><code>fixed_warm_up_iterations</code></strong></dt>
<dd>[int] Number of fixed steps to initilaise with.</dd>
<dt><strong><code>warm_up_repititions</code></strong></dt>
<dd>[int] Number of times to try for a better initial run.</dd>
<dt><strong><code>user_feedback</code></strong></dt>
<dd>[optional, bool] Whether or not to print progress.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>joint_model_chain</code></dt>
<dd>[chain] Generalised chain with states from any model.</dd>
<dt><code>total_acc</code></dt>
<dd>[list] Binary values, 1 if the state proposed was accepted,
0 if it was rejected, associated with the joint model.</dd>
<dt><code>inter_model_history</code></dt>
<dd>[model] Stored inter-model covariances and acc.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ARJMH(models, iterations,  adaptive_warm_up_iterations, fixed_warm_up_iterations = 25, warm_up_repititions = 1, user_feedback = False):
    &#34;&#34;&#34;Samples from a joint distribution of models.
    
    Initialises each model with multiple adaptive MH runs. Then uses the resulting
    covariances to run adaptive RJMH on all models.

    Args:
        models: [list] Model objects to sample from. 
                    Should be sorted by increasing dimensionality.
        iterations: [int] Number of adaptive RJMH steps.
        adaptive_warm_up_iterations: [int] Number of adaptive steps to initialise with.
        fixed_warm_up_iterations: [int] Number of fixed steps to initilaise with.
        warm_up_repititions: [int] Number of times to try for a better initial run.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        joint_model_chain: [chain] Generalised chain with states from any model.
        total_acc: [list] Binary values, 1 if the state proposed was accepted,
                          0 if it was rejected, associated with the joint model.
        inter_model_history: [model] Stored inter-model covariances and acc.
    &#34;&#34;&#34;

    if len(models) == 2:
        inter_model_history = deepcopy(models[1])
        inter_model_history.covariances = [schur_complement(models[1].covariance, models[0].D)]
    else: inter_model_history = None

    # Initialise model chains.
    for m_i in range(len(models)):
        models[m_i] = warm_up_model(models[m_i], adaptive_warm_up_iterations, fixed_warm_up_iterations, warm_up_repititions, user_feedback)

    random.seed(42)

    # Choose a random model to start in.
    model = random.choice(models)
    theta = deepcopy(model.sampled.states[-1]) # Final state in model&#39;s warmup chain.

    v = deepcopy(models[-1].sampled.states[-1]) # Auxiliary variables final state in super set model.
    lv = models[-1].sampled.states[-1].scaled - models[-1].centre.scaled # Auxiliary variables offset from centre.

    # Create joint model as initial theta appended to auxiliary variables.
    initial_superset = models[-1].D - model.D
    if initial_superset &gt; 0: # If random choice was a subset model
        theta_v = np.concatenate((theta.scaled, models[-1].sampled.states[-1].scaled[model.D:]))
        joint_model_chain = Chain(model.m, State(scaled = theta_v))
    else:
        joint_model_chain = Chain(model.m, theta)

    total_acc = np.zeros(iterations)
    total_acc[0] = 1

    v_D = models[-1].D # Dimension of largest model is auxilliary variable size.

    # Initial probability values.
    log_likelihood = model.log_likelihood(theta)
    log_prior = model.log_prior_density(theta, v = v, v_D = v_D)


    if user_feedback: print(&#34;Running ARJMH.&#34;)
    for i in range(1, iterations): # ARJMH algorithm.
        
        if user_feedback:
            cf = i / (iterations - 1)
            print(f&#39;model: {model.m} progress: [{&#34;#&#34;*round(50*cf)+&#34;-&#34;*round(50*(1-cf))}] {100.*cf:.2f}%\r&#39;, end=&#34;&#34;)

        # Propose a new model and state and calculate the resulting density.
        proposed_model = random.choice(models)
        proposed = State(scaled = ARJMH_proposal(model, proposed_model, theta, lv))
        log_likelihood_proposed = proposed_model.log_likelihood(proposed)
        log_prior_proposed = proposed_model.log_prior_density(proposed, v = v, v_D = v_D)

        # Metropolis acceptance criterion.
        if random.random() &lt; np.exp(log_likelihood_proposed - log_likelihood + log_prior_proposed - log_prior):
            # Accept proposal.
            total_acc[i] = 1

            if model is proposed_model: # Intra model move.
                model.acc.append(1)
            elif inter_model_history is not None: # Inter model move.
                inter_model_history.acc.append(1)
                inter_model_history.covariances.append(schur_complement(models[1].covariance, models[0].D))
                inter_model_history.sampled.n += 1

            model = proposed_model
            theta = deepcopy(proposed)

            log_likelihood = log_likelihood_proposed
            log_prior = log_prior_proposed
            
        else: # Reject proposal.
            total_acc[i] = 0
            
            if model is proposed_model: # Intra model move.
                model.acc.append(0)
            elif inter_model_history is not None: # Inter model move.
                inter_model_history.acc.append(0)
                inter_model_history.covariances.append(schur_complement(models[1].covariance, models[0].D))
                inter_model_history.sampled.n += 1
        
        # Update model chain.
        model.add_state(theta, adapt = True)
        v = State(scaled = np.concatenate((theta.scaled, v.scaled[model.D:])))
        joint_model_chain.add_general_state(model.m, v)

        # Update auxilliary centre divergence for new states.
        lv[:model.D] = theta.scaled - model.centre.scaled

    if user_feedback:
        print(f&#34;\n average acc: {np.average(total_acc):4f}&#34;)
        print(&#34;P(m1|y): &#34; + str(1 - np.sum(joint_model_chain.model_indices) / iterations))
        print(&#34;P(m2|y): &#34; + str(np.sum(joint_model_chain.model_indices) / iterations))

    return joint_model_chain, total_acc, inter_model_history</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.ARJMH_proposal"><code class="name flex">
<span>def <span class="ident">ARJMH_proposal</span></span>(<span>model, proposed_model, theta, lv)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs an Adaptive Reversible-Jump Metropolis Hastings proposal.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong></dt>
<dd>[model] Model to jump from.</dd>
<dt><strong><code>proposed_model</code></strong></dt>
<dd>[model] Model to jump to.</dd>
<dt><strong><code>theta</code></strong></dt>
<dd>[state] State to jump from.</dd>
<dt><strong><code>lv</code></strong></dt>
<dd>[list] Current auxilliary variables centre divergence.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>proposed_theta</code></dt>
<dd>[state] State proposed to jump to.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ARJMH_proposal(model, proposed_model, theta, lv):
    &#34;&#34;&#34;Performs an Adaptive Reversible-Jump Metropolis Hastings proposal.
    
    Args:
        model: [model] Model to jump from.
        proposed_model: [model] Model to jump to.
        theta: [state] State to jump from.
        lv: [list] Current auxilliary variables centre divergence.

    Returns:
        proposed_theta: [state] State proposed to jump to.
    &#34;&#34;&#34;
    l = theta.scaled - model.centre.scaled # Offset from initial model&#39;s centre.

    if model is proposed_model: # Intra-model move.

        # Use the covariance at the proposed model&#39;s centre for local shape.
        u = gaussian_proposal(np.zeros((proposed_model.D)), proposed_model.covariance)
        proposed_theta = u + l + proposed_model.centre.scaled
        
        return proposed_theta

    else: # Inter-model move.
        
        s = abs(model.D - proposed_model.D) # Subset size.

        # Use superset model covariance
        if proposed_model.D &gt; model.D: # proposed is superset
            cov = proposed_model.covariance
        else: # proposed is subset
            cov = model.covariance

        conditioned_cov = schur_complement(cov, s)

        # Jump to smaller model. Fix non-shared parameters.
        if proposed_model.D &lt; model.D:

            u = gaussian_proposal(np.zeros((s)), conditioned_cov)
            proposed_theta = u + l[:s] + proposed_model.centre.scaled

            return proposed_theta

        if proposed_model.D &gt; model.D: # Jump to larger model. Append v.

            u = gaussian_proposal(np.zeros((s)), conditioned_cov)
            shared_map = u + l[:s] + proposed_model.centre.scaled[:s]
            non_shared_map = lv[s:] + proposed_model.centre.scaled[s:]
            map = np.concatenate((shared_map, non_shared_map))
            proposed_theta = map

            return proposed_theta</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.check_symmetric"><code class="name flex">
<span>def <span class="ident">check_symmetric</span></span>(<span>A, tol=1e-16)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_symmetric(A, tol = 1e-16):
    return np.all(np.abs(A-A.T) &lt; tol)</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.gaussian_proposal"><code class="name flex">
<span>def <span class="ident">gaussian_proposal</span></span>(<span>theta, covariance)</span>
</code></dt>
<dd>
<div class="desc"><p>Samples a gaussian move.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gaussian_proposal(theta, covariance):
    &#34;&#34;&#34;Samples a gaussian move.&#34;&#34;&#34;
    return multivariate_normal.rvs(mean = theta, cov = covariance)</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.iterative_covariance"><code class="name flex">
<span>def <span class="ident">iterative_covariance</span></span>(<span>cov, x, x_mu, n, s, I, eps=1e-12)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iterative_covariance(cov, x, x_mu, n, s, I, eps = 1e-12):
    return (n-1)/n * cov + s/(n+1) * np.outer(x - x_mu, x - x_mu) + s*eps*I/n</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.iterative_mean"><code class="name flex">
<span>def <span class="ident">iterative_mean</span></span>(<span>x_mu, x, n)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iterative_mean(x_mu, x, n):
    return (x_mu * n + x)/(n + 1)</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.output_file"><code class="name flex">
<span>def <span class="ident">output_file</span></span>(<span>models, warm_up_iterations, joint_model_chain, total_acc, n_epochs, sn, letters, name='', event_params=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def output_file(models, warm_up_iterations, joint_model_chain, total_acc, n_epochs, sn, letters, name = &#34;&#34;, event_params = None):
    
    # Output File.
    with open(&#34;results/&#34;+name+&#34;-run.txt&#34;, &#34;w&#34;) as file:
        file.write(&#34;Run &#34;+name+&#34;\n&#34;)
        
        # Inputs.
        file.write(&#34;Inputs:\n&#34;)
        if event_params is not None:
            file.write(&#34;Parameters: &#34;+str(event_params.truth)+&#34;\n&#34;)
        file.write(&#34;Number of observations: &#34;+str(n_epochs)+&#34;, Signal to noise baseline: &#34;+str(sn)+&#34;\n&#34;)
        file.write(&#34;\n&#34;)
        file.write(&#34;Run information:\n&#34;)
        file.write(&#34;Iterations: &#34;+str(joint_model_chain.n)+&#34;\n&#34;)
        file.write(&#34;Average acc; Total: &#34;+str(np.average(total_acc)))

        # Results.
        file.write(&#34;\n\nResults:\n&#34;)
        for model in models:
            
            # Models.
            P_model = (model.sampled.n-warm_up_iterations)/joint_model_chain.n
            sde_model = np.std(np.array(joint_model_chain.model_indices))/(joint_model_chain.n**0.5)
            file.write(&#34;\n&#34;+str(model.m)+&#34;\nP(m|y): &#34;+str(P_model)+r&#34;\pm&#34;+str(sde_model)+&#34;\n&#34;)

            # Parameters.
            model_states = model.sampled.states_array(scaled = True)
            for i in range(len(model.sampled.states[-1].scaled)):
                mu = np.average(model_states[i, :])
                sd = np.std(model_states[i, :], ddof = 1)
                file.write(letters[i]+&#34;: mean: &#34;+str(mu)+&#34;, sd: &#34;+str(sd)+&#34; \n&#34;)
                
                # Hardcoded unscale q.
                if i == 3:
                    exp10 = np.power(10, model_states[i, :])
                    mu = np.average(exp10)
                    sd = np.std(exp10, ddof = 1)
                    file.write(&#34;q: mean: &#34;+str(mu)+&#34;, sd: &#34;+str(sd)+&#34; \n&#34;)

    return</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.schur_complement"><code class="name flex">
<span>def <span class="ident">schur_complement</span></span>(<span>cov, s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def schur_complement(cov, s):
    c_11 = cov[:s, :s] # Covariance matrix of shared parameters.
    c_12 = cov[:s, s:] # Covariances, not variances.
    c_21 = cov[s:, :s] # Same as above.
    c_22 = cov[s:, s:] # Covariance matrix of non-shared.
    c_22_inv = np.linalg.inv(c_22)

    return c_11 - c_12.dot(c_22_inv).dot(c_21)</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.warm_up_model"><code class="name flex">
<span>def <span class="ident">warm_up_model</span></span>(<span>empty_model, adaptive_iterations, fixed_iterations=25, repetitions=1, user_feedback=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Prepares a model for the ARJMH algorithm.</p>
<p>Repeats the adaptive MH warmup process for a model, storing the best run.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>empty_model</code></strong></dt>
<dd>[model] Initial model object.</dd>
<dt><strong><code>adaptive_iterations</code></strong></dt>
<dd>[int] Number of adaptive steps.</dd>
<dt><strong><code>fixed_iterations</code></strong></dt>
<dd>[optional, int] Number of non-adaptive steps.</dd>
<dt><strong><code>repetitions</code></strong></dt>
<dd>[optional, int] Number of times to try for a better run.</dd>
<dt><strong><code>user_feedback</code></strong></dt>
<dd>[optional, bool] Whether or not to print progress.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>incumbent_model</code></dt>
<dd>[model] Model with the states from the best run.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warm_up_model(empty_model, adaptive_iterations, fixed_iterations = 25, repetitions = 1, user_feedback = False):
    &#34;&#34;&#34;Prepares a model for the ARJMH algorithm.
    
    Repeats the adaptive MH warmup process for a model, storing the best run.

    Args:
        empty_model: [model] Initial model object.
        adaptive_iterations: [int] Number of adaptive steps.
        fixed_iterations: [optional, int] Number of non-adaptive steps.
        repetitions: [optional, int] Number of times to try for a better run.
        user_feedback: [optional, bool] Whether or not to print progress.

    Returns:
        incumbent_model: [model] Model with the states from the best run.
    &#34;&#34;&#34;

    inc_log_best_posterior = -math.inf # Initialise incumbent posterior to always lose.

    for i in range(repetitions):
        
        if user_feedback:
            print(&#34;Running the &#34;+str(i+1)+&#34;/&#34;+str(repetitions)+&#34;th initialisation per model\n&#34;)

        model = deepcopy(empty_model) # Fresh model.

        # Run adaptive MH.
        best_theta, log_best_posterior = AMH(model, adaptive_iterations, fixed_iterations = fixed_iterations, user_feedback = user_feedback)

        # Keep the best posterior density run.
        if inc_log_best_posterior &lt; log_best_posterior:
            incumbent_model = deepcopy(model)
            incumbent_model.centre = deepcopy(best_theta)

    return incumbent_model</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="bayelens.sampling.Chain"><code class="flex name class">
<span>class <span class="ident">Chain</span></span>
<span>(</span><span>m, state)</span>
</code></dt>
<dd>
<div class="desc"><p>Collection of states.</p>
<p>Describes a markov chain, perhaps from a joint model space.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>states</code></strong></dt>
<dd>[list] State objects in the chain.</dd>
<dt><strong><code>model_indices</code></strong></dt>
<dd>[list] models the states are from. </dd>
<dt><strong><code>n</code></strong></dt>
<dd>[int] The number of states in the chain.</dd>
</dl>
<p>Initialises the chain with one state from one model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>[state] The state object.</dd>
<dt><strong><code>m</code></strong></dt>
<dd>[int] The index of the model the state is from.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Chain(object):
    &#34;&#34;&#34;Collection of states.

    Describes a markov chain, perhaps from a joint model space.

    Attributes:
        states: [list] State objects in the chain.
        model_indices: [list] models the states are from. 
        n: [int] The number of states in the chain.
    &#34;&#34;&#34;

    def __init__(self, m, state):
        &#34;&#34;&#34;Initialises the chain with one state from one model.

        Args:
            state: [state] The state object.
            m: [int] The index of the model the state is from.
        &#34;&#34;&#34;
        self.states = [state]
        self.model_indices = [m]
        self.n = 1

    def add_general_state(self, m, state):
        &#34;&#34;&#34;Adds a state in a model to the chain.

        Args:
            state: [state] The state object.
            m: [int] The index of the model the state is from.
        &#34;&#34;&#34;
        self.states.append(state)
        self.model_indices.append(m)
        self.n += 1
        return

    def states_array(self, scaled = True):
        &#34;&#34;&#34;Creates a numpy array of all states in the chain.

        Args:
            scale: [optional, bool] Whether the array should be in scaled or 
                                    true space.

        Returns:
            chain_array: [np.array] The numpy array of all state parameters. 
                                    Columns are states, rows are parameters 
                                    for all states.
        &#34;&#34;&#34;
        n_states = len(self.states)
        D_state = len(self.states[-1].scaled)
        
        chain_array = np.zeros((D_state, n_states))

        if scaled:
            for i in range(n_states):
                chain_array[:, i] = self.states[i].scaled

        else:
            for i in range(n_states):
                chain_array[:, i] = self.states[i].truth

        return chain_array</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bayelens.sampling.Chain.add_general_state"><code class="name flex">
<span>def <span class="ident">add_general_state</span></span>(<span>self, m, state)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a state in a model to the chain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>state</code></strong></dt>
<dd>[state] The state object.</dd>
<dt><strong><code>m</code></strong></dt>
<dd>[int] The index of the model the state is from.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_general_state(self, m, state):
    &#34;&#34;&#34;Adds a state in a model to the chain.

    Args:
        state: [state] The state object.
        m: [int] The index of the model the state is from.
    &#34;&#34;&#34;
    self.states.append(state)
    self.model_indices.append(m)
    self.n += 1
    return</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.Chain.states_array"><code class="name flex">
<span>def <span class="ident">states_array</span></span>(<span>self, scaled=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a numpy array of all states in the chain.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>scale</code></strong></dt>
<dd>[optional, bool] Whether the array should be in scaled or
true space.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>chain_array</code></dt>
<dd>[np.array] The numpy array of all state parameters.
Columns are states, rows are parameters
for all states.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def states_array(self, scaled = True):
    &#34;&#34;&#34;Creates a numpy array of all states in the chain.

    Args:
        scale: [optional, bool] Whether the array should be in scaled or 
                                true space.

    Returns:
        chain_array: [np.array] The numpy array of all state parameters. 
                                Columns are states, rows are parameters 
                                for all states.
    &#34;&#34;&#34;
    n_states = len(self.states)
    D_state = len(self.states[-1].scaled)
    
    chain_array = np.zeros((D_state, n_states))

    if scaled:
        for i in range(n_states):
            chain_array[:, i] = self.states[i].scaled

    else:
        for i in range(n_states):
            chain_array[:, i] = self.states[i].truth

    return chain_array</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bayelens.sampling.Model"><code class="flex name class">
<span>class <span class="ident">Model</span></span>
<span>(</span><span>m, D, centre, priors, covariance, data, log_likelihood_fnc)</span>
</code></dt>
<dd>
<div class="desc"><p>A model to describe a probability distribution.</p>
<p>Contains a chain of states from this model, as well as information
from this. Adapts a covariance matrix iteratively with each new state,
and stores a guess at a maximum posterior density estimate.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>m</code></strong></dt>
<dd>[int] Model index.</dd>
<dt><strong><code>D</code></strong></dt>
<dd>[int] Dimensionality of a state in the model.</dd>
<dt><strong><code>priors</code></strong></dt>
<dd>[list] Prior distribution objects for state parameter values.</dd>
<dt><strong><code>sampled</code></strong></dt>
<dd>[chain] States sampled from the model's distribution.</dd>
<dt><strong><code>scaled_average_state</code></strong></dt>
<dd>[list] The scaled average parameter values of the chain.</dd>
<dt><strong><code>centre</code></strong></dt>
<dd>[state] Best guess at maximum posterior density.</dd>
<dt><strong><code>covariance</code></strong></dt>
<dd>[array] Current covariance matrix, based on all states.</dd>
<dt><strong><code>covariances</code></strong></dt>
<dd>[list] All previous covariance matrices.</dd>
<dt><strong><code>acc</code></strong></dt>
<dd>[list] Binary values, 1 if the state proposed was accepted,
0 if it was rejected.</dd>
<dt><strong><code>data</code></strong></dt>
<dd>[mulensdata] Object for photometry readings from the
microlensing event.</dd>
<dt><strong><code>log_likelihood</code></strong></dt>
<dd>[function] Method to calculate the log likelihood a state is
from this model.</dd>
<dt><strong><code>I</code></strong></dt>
<dd>[np.array] Identity matrix the size of D.</dd>
<dt><strong><code>s</code></strong></dt>
<dd>[float] Mixing parameter (see Haario et al 2001).</dd>
</dl>
<p>Initialises the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Model(object):
    &#34;&#34;&#34;A model to describe a probability distribution.

    Contains a chain of states from this model, as well as information
    from this. Adapts a covariance matrix iteratively with each new state,
    and stores a guess at a maximum posterior density estimate.

    Attributes:
        m: [int] Model index.
        D: [int] Dimensionality of a state in the model.
        priors: [list] Prior distribution objects for state parameter values.
        sampled: [chain] States sampled from the model&#39;s distribution.
        scaled_average_state: [list] The scaled average parameter values of the chain.
        centre: [state] Best guess at maximum posterior density.
        covariance: [array] Current covariance matrix, based on all states.
        covariances: [list] All previous covariance matrices.
        acc: [list] Binary values, 1 if the state proposed was accepted,
                    0 if it was rejected.
        data: [mulensdata] Object for photometry readings from the 
            microlensing event.
        log_likelihood: [function] Method to calculate the log likelihood a state is
                                    from this model.
        I: [np.array] Identity matrix the size of D.
        s: [float] Mixing parameter (see Haario et al 2001).
    &#34;&#34;&#34;

    def __init__(self, m, D, centre, priors, covariance, data, log_likelihood_fnc):
        &#34;&#34;&#34;Initialises the model.&#34;&#34;&#34;
        self.m = m
        self.D = D
        self.priors = priors
        self.centre = centre
        self.sampled = Chain(m, centre)
        self.scaled_avg_state = centre.scaled
        self.acc = [1] # First state always accepted.
        self.covariance = covariance
        self.covariances = [covariance]

        self.data = data
        
        # Model&#39;s custom likelihood function.
        self.log_likelihood = MethodType(log_likelihood_fnc, self)

        self.I = np.identity(D)
        self.s = 2.4**2 / D # Arbitrary(ish), good value from Haario et al 2001.
    
    def add_state(self, theta, adapt = True):
        &#34;&#34;&#34;Adds a sampled state to the model.

        Args:
            theta: [state] Parameters to add.
            adapt: [optional, bool] Whether or not to adjust the covariance 
                                    matrix based on the new state.
        &#34;&#34;&#34;
        self.sampled.n += 1
        self.sampled.states.append(theta)

        if adapt:
            self.covariance = iterative_covariance(self.covariance, theta.scaled, self.scaled_avg_state, self.sampled.n, self.s, self.I)

        self.covariances.append(self.covariance)
        self.scaled_avg_state = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n)
        self.centre = State(scaled = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n))

        return

    def log_prior_density(self, theta, v = None, v_D = None):
        &#34;&#34;&#34;Calculates the log prior density of a state in the model.

        Optionally adjusts this log density when using auxilliary vriables.

        Args:
            theta: [state] Parameters to calculate the log prior density for.
            v: [optional, state] The values of all auxiliary variables.
            v_D: [optional, int] The dimensionality to use with auxilliary variables.

        Returns:
            log_prior_product: [float] The log prior probability density.
        &#34;&#34;&#34;    
        log_prior_product = 0.

        # cycle through parameters
        for p in range(self.D):

            # product using log rules
            log_prior_product += (self.priors[p].log_pdf(theta.truth[p]))

        # cycle through auxiliary parameters if v and v_D passed
        if v is not None or v_D is not None:
            if v is not None and v_D is not None:
                for p in range(self.D, v_D):
                    
                    # product using log rules
                    log_prior_product += (self.priors[p].log_pdf(v.truth[p]))

            else: raise ValueError(&#34;Only one of v or v_D passed.&#34;)

        return log_prior_product</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="bayelens.sampling.Model.add_state"><code class="name flex">
<span>def <span class="ident">add_state</span></span>(<span>self, theta, adapt=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds a sampled state to the model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>theta</code></strong></dt>
<dd>[state] Parameters to add.</dd>
<dt><strong><code>adapt</code></strong></dt>
<dd>[optional, bool] Whether or not to adjust the covariance
matrix based on the new state.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_state(self, theta, adapt = True):
    &#34;&#34;&#34;Adds a sampled state to the model.

    Args:
        theta: [state] Parameters to add.
        adapt: [optional, bool] Whether or not to adjust the covariance 
                                matrix based on the new state.
    &#34;&#34;&#34;
    self.sampled.n += 1
    self.sampled.states.append(theta)

    if adapt:
        self.covariance = iterative_covariance(self.covariance, theta.scaled, self.scaled_avg_state, self.sampled.n, self.s, self.I)

    self.covariances.append(self.covariance)
    self.scaled_avg_state = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n)
    self.centre = State(scaled = iterative_mean(self.scaled_avg_state, theta.scaled, self.sampled.n))

    return</code></pre>
</details>
</dd>
<dt id="bayelens.sampling.Model.log_prior_density"><code class="name flex">
<span>def <span class="ident">log_prior_density</span></span>(<span>self, theta, v=None, v_D=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the log prior density of a state in the model.</p>
<p>Optionally adjusts this log density when using auxilliary vriables.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>theta</code></strong></dt>
<dd>[state] Parameters to calculate the log prior density for.</dd>
<dt><strong><code>v</code></strong></dt>
<dd>[optional, state] The values of all auxiliary variables.</dd>
<dt><strong><code>v_D</code></strong></dt>
<dd>[optional, int] The dimensionality to use with auxilliary variables.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>log_prior_product</code></dt>
<dd>[float] The log prior probability density.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_prior_density(self, theta, v = None, v_D = None):
    &#34;&#34;&#34;Calculates the log prior density of a state in the model.

    Optionally adjusts this log density when using auxilliary vriables.

    Args:
        theta: [state] Parameters to calculate the log prior density for.
        v: [optional, state] The values of all auxiliary variables.
        v_D: [optional, int] The dimensionality to use with auxilliary variables.

    Returns:
        log_prior_product: [float] The log prior probability density.
    &#34;&#34;&#34;    
    log_prior_product = 0.

    # cycle through parameters
    for p in range(self.D):

        # product using log rules
        log_prior_product += (self.priors[p].log_pdf(theta.truth[p]))

    # cycle through auxiliary parameters if v and v_D passed
    if v is not None or v_D is not None:
        if v is not None and v_D is not None:
            for p in range(self.D, v_D):
                
                # product using log rules
                log_prior_product += (self.priors[p].log_pdf(v.truth[p]))

        else: raise ValueError(&#34;Only one of v or v_D passed.&#34;)

    return log_prior_product</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="bayelens.sampling.State"><code class="flex name class">
<span>class <span class="ident">State</span></span>
<span>(</span><span>truth=None, scaled=None)</span>
</code></dt>
<dd>
<div class="desc"><p>State sampled from a model's probability distribution.</p>
<p>Describes a point in both scaled and unscaled space. The scaling is
hardcoded but can be extended per application. Currently log10 scaling
the fourth parameter. In microlensing applications, this is q,
the mass ratio.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>truth</code></strong></dt>
<dd>[list] Parameter values for the state, in true space.</dd>
<dt><strong><code>scaled</code></strong></dt>
<dd>[list] Parameter values for the state, in scaled space.</dd>
<dt><strong><code>D</code></strong></dt>
<dd>[int] The dimensionality of the state.</dd>
</dl>
<p>Initialises state with truth, scaled, and D values.</p>
<p>Only one of truth or scaled is needed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class State(object):
    &#34;&#34;&#34;State sampled from a model&#39;s probability distribution.

    Describes a point in both scaled and unscaled space. The scaling is 
    hardcoded but can be extended per application. Currently log10 scaling 
    the fourth parameter. In microlensing applications, this is q,
    the mass ratio.

    Attributes:
        truth: [list] Parameter values for the state, in true space.
        scaled: [list] Parameter values for the state, in scaled space.
        D: [int] The dimensionality of the state.
    &#34;&#34;&#34;

    def __init__(self, truth = None, scaled = None):
        &#34;&#34;&#34;Initialises state with truth, scaled, and D values.
        
        Only one of truth or scaled is needed.
        &#34;&#34;&#34;        
        if truth is not None:
            self.truth = truth
            self.D = len(truth)

            self.scaled = deepcopy(self.truth)
            for p in range(self.D):
                if p == 3:
                    self.scaled[p] = np.log10(self.truth[p])
        
        elif scaled is not None:
            self.scaled = scaled
            self.D = len(scaled)

            self.truth = deepcopy(self.scaled)
            for p in range(self.D):
                if p == 3:
                    self.truth[p] = 10**(self.scaled[p])

        else:   raise ValueError(&#34;Assigned null state&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="bayelens" href="index.html">bayelens</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="bayelens.sampling.AMH" href="#bayelens.sampling.AMH">AMH</a></code></li>
<li><code><a title="bayelens.sampling.ARJMH" href="#bayelens.sampling.ARJMH">ARJMH</a></code></li>
<li><code><a title="bayelens.sampling.ARJMH_proposal" href="#bayelens.sampling.ARJMH_proposal">ARJMH_proposal</a></code></li>
<li><code><a title="bayelens.sampling.check_symmetric" href="#bayelens.sampling.check_symmetric">check_symmetric</a></code></li>
<li><code><a title="bayelens.sampling.gaussian_proposal" href="#bayelens.sampling.gaussian_proposal">gaussian_proposal</a></code></li>
<li><code><a title="bayelens.sampling.iterative_covariance" href="#bayelens.sampling.iterative_covariance">iterative_covariance</a></code></li>
<li><code><a title="bayelens.sampling.iterative_mean" href="#bayelens.sampling.iterative_mean">iterative_mean</a></code></li>
<li><code><a title="bayelens.sampling.output_file" href="#bayelens.sampling.output_file">output_file</a></code></li>
<li><code><a title="bayelens.sampling.schur_complement" href="#bayelens.sampling.schur_complement">schur_complement</a></code></li>
<li><code><a title="bayelens.sampling.warm_up_model" href="#bayelens.sampling.warm_up_model">warm_up_model</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="bayelens.sampling.Chain" href="#bayelens.sampling.Chain">Chain</a></code></h4>
<ul class="">
<li><code><a title="bayelens.sampling.Chain.add_general_state" href="#bayelens.sampling.Chain.add_general_state">add_general_state</a></code></li>
<li><code><a title="bayelens.sampling.Chain.states_array" href="#bayelens.sampling.Chain.states_array">states_array</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bayelens.sampling.Model" href="#bayelens.sampling.Model">Model</a></code></h4>
<ul class="">
<li><code><a title="bayelens.sampling.Model.add_state" href="#bayelens.sampling.Model.add_state">add_state</a></code></li>
<li><code><a title="bayelens.sampling.Model.log_prior_density" href="#bayelens.sampling.Model.log_prior_density">log_prior_density</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="bayelens.sampling.State" href="#bayelens.sampling.State">State</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>